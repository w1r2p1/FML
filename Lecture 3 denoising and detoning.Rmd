---
title: "Algorithmic trading and investment"
subtitle: "FIN7030"
author: "Barry Quinn"
date: "2019/01/15 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["default", "css/sfah.css", "css/fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "https://platform.twitter.com/widgets.js"
    seal: false 
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# options(knitr.table.format = "html")
library(tidyverse)
library(babynames)
library(fontawesome) 
library(DiagrammeR)
```

layout: true
  
<div class="my-footer"><span>quinference.com</span></div>

```{r xaringan-logo, echo=FALSE}
xaringanExtra::use_logo(
  image_url = "img/redlogo.png"
)
```

```{r xaringanExtra, include = FALSE}
xaringanExtra::use_xaringan_extra(c("tileview", "webcam","panelset","tachuyon"))
```

---
name: ATL-title
class: inverse,left, middle
background-image: url(img/title-slide-img.png)
background-size: cover

# Algorithmic Trading and Investing (FIN7030)

# .fancy[Lecture 3: Denoising and detoning]

.large[Barry Quinn PhD CStat | Queen's Management School | `r Sys.Date()`]

<!-- this ends up being the title slide since seal = FALSE-->

---
class:inverse
# Outline 

- .large[Denoising and detoning financial covariance matrices]
- .large[Marcenko-Pastur Theorem]
- .large.fancy[Breathing breaks to aid learning]
---
class: inverse
## Teaching mathematical concepts through code

- Understanding machine learning requires familiarity with many simple mathematical concepts
- **Some examples**
- The goal of this course is to build your intuition about these notions without getting overly technical
- In particular I will steer away from mathematical notation, which can be off-putting for those without any mathematics background and isn't strictly necessary to explain things well.
- Mostly, we will use coded practical example to illustrate the concepts
- This is a .large.fancy[**learning by doing**] approach when you can play around with the code yourself you understand underlying concepts.
---
class: middle
# Why study denoising and detoning?

- .large[Covariance matrices are everywhere in finance.]
- .large[Empirical covariances measure the linear co-movement between a set of random variables]  
- .large[For example to estimate the linear comovement between FTSE 100 stocks you would gather 100 time series of each stocks returns]
- .large[They are used to:]

1. Run regressions
2. Estimate risk
3. Optimise portfolios
4. Simulate scenarios via Monte Carlo
5. Find clusters
6. Reduce the dimensionality of a set of potential predictors

---
class: middle 
# Why study denoising and detoning?

- .large[Empirical covariance matrices are estimated with flawed, incomplete data which leads to estimates with an amount of noise]

- .large[Such noise can render calculations using covariance matrix estimates useless]

 .large[ In finance, we need a procedure to reduce this noise and enhance the signal **before** using in subsequent analysis like those listed previously]

---
class: middle
# The Marcenko-Pastur Theorem
- This is an elegant mathematical theorem that can help *denoise* data by distinguishing random from non-random from nonrandom by means of the empirical correlation matrix of the data.

- The probability density function (pdf) of a Marcenko-Pastur distribution produces a set of eigenvalues which are randomly distributed.

- We can exploit this property to extract the non-random component of some empirical correlation matrix, thus removing the random noise.
---
---
class: top
## The Marcenko-Pastur Theorem in Python
.panelset[
.panel[.panel-name[ Python + R set-up]
```{r set-up}
library(reticulate)
```

- *Insert Python code chunk in RMarkdown file*
]
.panel[.panel-name[Define the Marcenko-Pastur PDF]
```{python}
import numpy as np
import pandas as pd
# ------------------------------
def mpPDF(var,q,pts):
  eMin,eMax=var*(1-(1./q)**.5)**2,var*(1+(1./q)**.5)**2
  eVal=np.linspace(eMin,eMax,pts)
  pdf=q/(2*np.pi*var*eVal)*((eMax-eVal)*(eVal-eMin))**.5
  pdf=pd.Series(pdf,index=eVal)
  return pdf
```
]
.panel[.panel-name[Define a functions to test theorem]

- This function is a principal component analysis function which will extract the Eigenvalues (principal components)
```{python}
from sklearn.neighbors.kde import KernelDensity
##-----------------
def getPCA(matrix):
# Get eVal,eVec from a Hermitian Matrix
  eVal,eVec=np.linalg.eigh(matrix)
  indices=eVal.argsort()[::-1] # arguments for sorting eVal in descending order
  eVal,eVec=eVal[indices],eVec[:,indices]
  eVal=np.diagflat(eVal)
  return eVal,eVec
```

- This function create a pdf for a series of supplied observations (obs)

```{python}
##---------------
def fitKDE(obs,bWidth=.25,kernel='gaussian',x=None):
# Fit kernel to a series of obs, and derive the probability of obs
# x is the array of values on which to fit KDE will be evaluate
  if len(obs.shape)==1:obs=obs.reshape(-1,1)
  kde=KernelDensity(kernel=kernel,bandwidth=bWidth).fit(obs)
  if x is None:x=np.unique(obs).reshape(-1,1)
  if len(x.shape)==1:x=x.reshape(-1,1)
  logProb=kde.score_samples(x) # log of probability density
  pdf=pd.Series(np.exp(logProb),index=x.flatten())
  return pdf
```
]
.panel[.panel-name[Test the theorem]

```{python}
# Create a random sample for testing
x=np.random.normal(size=(10000,1000))
# Extract correlation matrix from the random sample
eVal0,eVec0=getPCA(np.corrcoef(x,rowvar=0))
# Create theoretical Marcenko-Pastur distribution of the random sample x
pdf0=mpPDF(1.,q=x.shape[0]/float(x.shape[1]),pts=1000)
# Create the empirical pdf of x
pdf1=fitKDE(np.diag(eVal0),bWidth=.01)

```
]
.panel[.panel-name[Visualising Marcenko-Pastur theorem]
```{r plot pdfs}
pdf_mp=py$pdf0
pdf_emp=py$pdf1
```

]
]
---


